---
title: 搭建Presto0.231.1+Hive2.3.6+Hadoop2.7.7
tags: hadoop2.7.7 hive2.3.6 presto0.231.1
key: 2020-02-18-搭建Presto
---

前期基于Hadoop3.2.1和Hive3.1.2搭建了集群，由于Presto能够满足大数据下的即席查询效率，因此需要安装Presto，但
Presto目前只支持Hive2.x，没办法只能重新搭建Hive和Hadoop环境。

项目github: https://github.com/JohnWey/Hadoop_Hive_Presto

#### 1. 搭建Hadoop2.7.7


<div class="article__content" markdown="1">

创建hadoop的DockerFile，包括JDK和OPENSSH：

```
FROM debian:9

MAINTAINER JohnWey <john.w.zhang@yzw.cn>

COPY sources.list /etc/apt/sources.list

RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
      vim \
      openjdk-8-jdk \
      openssh-server \
      net-tools \
      curl \
      netcat \
    && rm -rf /var/lib/apt/lists/* && apt-get autoclean

ENV HADOOP_VERSION 2.7.7

# install hadoop
ADD hadoop-${HADOOP_VERSION}.tar.gz /opt

# set environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/opt/hadoop-${HADOOP_VERSION}
ENV PATH=$PATH:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
RUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop
ENV HADOOP_CONF_DIR=/etc/hadoop


# ssh without key
RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


# create directory
RUN mkdir -p /hadoop-data/dfs/name && \
    mkdir -p /hadoop-data/dfs/data && \
    mkdir $HADOOP_HOME/logs

COPY config/* /tmp/

RUN mv /tmp/ssh_config ~/.ssh/config && \
    mv /tmp/hadoop-env.sh ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    mv /tmp/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml && \
    mv /tmp/core-site.xml ${HADOOP_HOME}/etc/hadoop/core-site.xml && \
    mv /tmp/mapred-site.xml ${HADOOP_HOME}/etc/hadoop/mapred-site.xml && \
    mv /tmp/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/yarn-site.xml && \
    mv /tmp/workers ${HADOOP_HOME}/etc/hadoop/workers && \
    mv /tmp/start-hadoop.sh ~/start-hadoop.sh && \
    mv /tmp/stop-hadoop.sh ~/stop-hadoop.sh && \
    mv /tmp/run-wordcount.sh ~/run-wordcount.sh && \
    rm -rf /tmp && \
    sed -i 's/mesg n || true/tty -s \&\& mesg n/g'  /root/.profile && \
    # grant privilege
    chmod +x ~/start-hadoop.sh && \
    chmod +x ~/run-wordcount.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-dfs.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-yarn.sh

# format namenode
RUN ${HADOOP_HOME}/bin/hdfs namenode -format

#CMD [ "/usr/sbin/sshd", "-D"; bash"]
CMD [ "sh", "-c", "service ssh start; bash"]

```

上述配置文件配置如下

> **core-site.xml**

```
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop2-bimaster:9000</value>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/hadoop/hdfs/tmp</value>
    </property>

    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>

    <property>
	<name>hadoop.proxyuser.root.hosts</name>
	<value>*</value>
    </property>
    <property>
	<name>hadoop.proxyuser.root.groups</name>
	<value>*</value>
    </property>

</configuration>

```
> **hdfs-site.xml**


```
<configuration>
 <property>
    <name>dfs.http.address</name>
    <value>hadoop2-bimaster:50070</value>
 </property>
 <property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/home/hadoop/hdfs/name</value>
   <final>true</final>
</property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/home/hadoop/hdfs/data</value>
   <final>true</final>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
   <value>hadoop2-bimaster:50090</value>
 </property>
 <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
 </property>
 <property>
   <name>dfs.permissions</name>
   <value>false</value>
 </property>
</configuration>

```

> **mapred-site.xml**

```
<configuration>
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

<property>
    <name>mapreduce.application.classpath</name>
    <value>
        /opt/hadoop-2.7.7/etc/hadoop,
        /opt/hadoop-2.7.7/share/hadoop/common/*,
        /opt/hadoop-2.7.7/share/hadoop/common/lib/*,
        /opt/hadoop-2.7.7/share/hadoop/hdfs/*,
        /opt/hadoop-2.7.7/share/hadoop/hdfs/lib/*,
        /opt/hadoop-2.7.7/share/hadoop/mapreduce/*,
        /opt/hadoop-2.7.7/share/hadoop/mapreduce/lib/*,
        /opt/hadoop-2.7.7/share/hadoop/yarn/*,
        /opt/hadoop-2.7.7/share/hadoop/yarn/lib/*
    </value>
</property>
</configuration>

```


> **yarn-site.xml**

```
<configuration>

<!-- Site specific YARN configuration properties -->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop2-bimaster</value>
    </property>
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>hadoop2-bimaster:8032</value>
    </property>
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>hadoop2-bimaster:8031</value>
    </property>
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>hadoop2-bimaster:8030</value>
    </property>
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>hadoop2-bimaster:8033</value>
    </property>
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>hadoop2-bimaster:8088</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>flase</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>6</value>
        <description>每个任务使用的虚拟内存占物理内存的百分比</description>
    </property>
</configuration>

```

> **workers**

```
hadoop2-bimaster
hadoop2-bislaver01
hadoop2-bislaver02
```

> **ssh_config**

```
Host localhost
  StrictHostKeyChecking no

Host 0.0.0.0
  StrictHostKeyChecking no

Host hadoop-*
   StrictHostKeyChecking no
```

> **hadoop-env.sh**

```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```
> **start-hadoop.sh**

```
#!/bin/bash

echo -e "\n"

$HADOOP_HOME/sbin/start-dfs.sh

echo -e "\n"

$HADOOP_HOME/sbin/start-yarn.sh

echo -e "\n"
```

> **stop-hadoop.sh**

```
#!/bin/bash

echo -e "\n"

$HADOOP_HOME/sbin/stop-yarn.sh

echo -e "\n"

$HADOOP_HOME/sbin/stop-dfs.sh

echo -e "\n"
```
> **run-wordcount.sh**

```
#!/bin/bash

# test the hadoop cluster by running wordcount

# create input files
mkdir input
echo "Hello Docker" >input/file2.txt
echo "Hello Hadoop" >input/file1.txt

# create input directory on HDFS
hadoop fs -mkdir -p input

# put input files to HDFS
hdfs dfs -put ./input/* input

# run wordcount
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount input output

# print the input files
echo -e "\ninput file1.txt:"
hdfs dfs -cat input/file1.txt

echo -e "\ninput file2.txt:"
hdfs dfs -cat input/file2.txt

# print the output of wordcount
echo -e "\nwordcount output:"
hdfs dfs -cat output/part-r-00000

```

使用下面的shell创建3个docker并搭建专用网络：

> **start_container**

```
#!/bin/bash

# the default node number is 3
N=${1:-3}


# start hadoop master container
sudo docker rm -f hadoop2-bimaster &> /dev/null
echo "start hadoop2-bimaster container..."
sudo docker run -itd \
                --net=hadoop2 \
                -p 9990:9000 \
                -p 59970:50070 \
                -p 18988:18088 \
                --name hadoop2-bimaster \
                --hostname hadoop2-bimaster \
                bidw/hadoop:base_v2.0 &> /dev/null


# start hadoop slave container
i=1
while [ $i -lt $N ]
do
	sudo docker rm -f hadoop2-bislaver0$i &> /dev/null
	echo "start hadoop2-bislaver0$i container..."
	sudo docker run -itd \
	                --net=hadoop2 \
	                --name hadoop2-bislaver0$i \
	                --hostname hadoop2-bislaver0$i \
	                bidw/hadoop:base_v2.0 &> /dev/null
	i=$(( $i + 1 ))
done

# get into hadoop master container
sudo docker exec -it hadoop2-bimaster bash

```

搭建hadoop2网络：

```
docker create network --driver=bridge hadoop2
```

删除hadoop2网络：

```
docker network rm hadoop2
```

</div>



#### 2. 搭建Hive2.3.6

<div class="article__content" markdown="1">

Dockerfile:

```
ROM bidw/hadoop:base_v2.0

MAINTAINER JohnWey <john.w.zhang@yzw.cn>

ENV HADOOP_VERSION 2.7.7
ENV HIVE_VERSION 2.3.6

# install hive
ADD apache-hive-${HIVE_VERSION}-bin.tar.gz /opt
RUN mv /opt/apache-hive-${HIVE_VERSION}-bin /opt/hive


# set environment variable
ENV HADOOP_HOME=/opt/hadoop-${HADOOP_VERSION}
ENV HIVE_HOME=/opt/hive
ENV HIVE_CONF_DIR=${HIVE_HOME}/conf
ENV PATH=$PATH:${JAVA_HOME}/bin:${HIVE_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin


COPY mysql-connector-java-5.1.47.jar ${HIVE_HOME}/lib

COPY config/* /tmp/

#Custom configuration goes here
RUN mv /tmp/hive-site.xml $HIVE_HOME/conf  && \
    mv /tmp/beeline-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp//hive-env.sh $HIVE_HOME/conf && \
    mv /tmp/hive-exec-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp/hive-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp/ivysettings.xml $HIVE_HOME/conf && \
    mv /tmp/llap-daemon-log4j2.properties $HIVE_HOME/conf && \
    #mv ${HIVE_HOME}/lib/guava-19.0.jar ${HIVE_HOME}/lib/guava-19.0.jar_bak && \
    #cp ${HADOOP_HOME}/share/hadoop/common/lib/guava-27.0-jre.jar ${HIVE_HOME}/lib/ && \
    mv /tmp/startup_hive.sh /usr/local/bin/ && \
    rm -rf /tmp && \
    chmod +x /usr/local/bin/startup_hive.sh

WORKDIR /usr/local/bin

#CMD [ "/usr/sbin/sshd", "-D"; bash"]
CMD [ "sh", "-c", "service ssh start; bash"]
```

> **hive-site.xml**

```
<configuration>
        <property>
                <name>hive.exec.scratchdir</name>
                <value>/home/hadoop/hive/tmp</value>
        </property>
        <property>
                <name>hive.metastore.warehouse.dir</name>
                <value>/home/hadoop/hive/data</value>
        </property>
        <property>
                <name>hive.querylog.location</name>
                <value>/opt/hive/log</value>
        </property>

	<property>
		<name>javax.jdo.option.ConnectionURL</name>
	        <value>jdbc:mysql://hive2-mysql:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>yzw@123</value>
	</property>
	<property>
		<name>datanucleus.readOnlyDatastore</name>
		<value>false</value>
	</property>
	<property>
		<name>datanucleus.fixedDatastore</name>
		<value>false</value>
	</property>
	<property>
		<name>datanucleus.autoCreateSchema</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.schema.autoCreateAll</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.autoCreateTables</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.autoCreateColumns</name>
		<value>true</value>
	</property>
	<!-- 显示表的列名 -->
	<property>
		<name>hive.cli.print.header</name>
		<value>true</value>
	</property>
	<!-- 显示数据库名称 -->
	<property>
		<name>hive.cli.print.current.db</name>
		<value>true</value>
	</property>
    <!-- Presto用该uris连接hive -->
    <property>
            <name>hive.metastore.uris</name>
            <value>thrift://hive2-mysql:9083</value>
    </property>
</configuration>

```

> **hive-env.sh**

```
HADOOP_HOME=/opt/hadoop-2.7.7
export HIVE_CONF_DIR=/opt/hive/conf
export HIVE_AUX_JARS_PATH=/opt/hive/lib
```

> **startup_hive.sh**


```
#!/bin/bash


hadoop fs -mkdir       /tmp
hadoop fs -mkdir -p    /user/hive/warehouse
hadoop fs -chmod g+w   /tmp
hadoop fs -chmod g+w   /user/hive/warehouse

schematool -dbType mysql -initSchema

#cd $HIVE_HOME/bin
#./hiveserver2 --hiveconf hive.server2.enable.doAs=false
```

运行hive container: sh start_hive_container.sh

```
#!/bin/bash


# start hive container
sudo docker rm -f hadoop2-hive &> /dev/null
echo "start hadoop2-hive container..."
sudo docker run -itd \
                --net=hadoop2 \
                -p 19900:10000 \
                --name hadoop2-hive \
                --hostname hadoop2-hive \
                bidw/hive:base_v2.0


sudo docker exec -it hadoop2-hive bash
```
</div>


#### 3. 搭建MySQL
直接拉取Mysql镜像即可：
```
docker pull mysql:5.7
```
运行镜像：
```
docker run -itd --net=hadoop2 --name hive-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=yzw@123 mysql:5.7
```


#### 4. Presto
> **hive.properties**
connector.name=hive-hadoop2
hive.metastore.uri=thrift://hadoop2-hive:9083
hive.config.resources=/opt/presto/etc/hadoop/core-site.xml,/opt/presto/etc/hadoop/hdfs-site.xml

注意，这里需要将core-site.xml和hdfs-site.xml从hadoop主节点copy到hive-coordinator节点

#### 5. QA
Q1：Query 20200228_073205_00010_24rrv failed: line 1:1: Catalog 'mysql' does not exist

A1：这有可能是因为设置完mysql.properties后没有重启Container;需要注意的是每个worker都需要配置mysql.properties

Q2：FAILED: SemanticException org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient

A2：这是因为在hive-site.xml中没有配置
    <property>
            <name>hive.metastore.uris</name>
            <value>thrift://hive2-mysql:9083</value>
    </property>
    且没有启动metastore服务。配置完成后需要手动启动metastore服务，同时重启hive container和presto-coordinator container。





