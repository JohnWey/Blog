---
title: 搭建Hadoop3.2.1+Hive3.1.2+Mysql5.7+Docker19
tags: hadoop3.2.1 hive3.1.2 mysql5.7 docker19
key: 2020-02-18-搭建Hadoop
---

#### 1. 搭建Hadoop3.2.1


<div class="article__content" markdown="1">

创建hadoop的DockerFile，包括JDK和OPENSSH：
```
FROM debian:9
MAINTAINER JohnWey
COPY sources.list /etc/apt/sources.list
RUN apt-get update && DEBIAN_FRONTEND=noninteractive apt-get install -y --no-install-recommends \
      vim \
      openjdk-8-jdk \
      openssh-server \
      net-tools \
      curl \
      netcat \
    && rm -rf /var/lib/apt/lists/* && apt-get autoclean

ENV HADOOP_VERSION 3.2.1

# install hadoop
ADD hadoop-${HADOOP_VERSION}.tar.gz /opt

# set environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV HADOOP_HOME=/opt/hadoop-${HADOOP_VERSION}
ENV PATH=$PATH:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
RUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop
ENV HADOOP_CONF_DIR=/etc/hadoop


# ssh without key
RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


# create directory
RUN mkdir -p /hadoop-data/dfs/name && \
    mkdir -p /hadoop-data/dfs/data && \
    mkdir $HADOOP_HOME/logs

COPY config/* /tmp/

RUN mv /tmp/ssh_config ~/.ssh/config && \
    mv /tmp/hadoop-env.sh ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    mv /tmp/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml && \
    mv /tmp/core-site.xml ${HADOOP_HOME}/etc/hadoop/core-site.xml && \
    mv /tmp/mapred-site.xml ${HADOOP_HOME}/etc/hadoop/mapred-site.xml && \
    mv /tmp/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/yarn-site.xml && \
    mv /tmp/workers ${HADOOP_HOME}/etc/hadoop/workers && \
    mv /tmp/start-hadoop.sh ~/start-hadoop.sh && \
    mv /tmp/stop-hadoop.sh ~/stop-hadoop.sh && \
    mv /tmp/run-wordcount.sh ~/run-wordcount.sh && \
    rm -rf /tmp && \
    sed -i 's/mesg n || true/tty -s \&\& mesg n/g'  /root/.profile && \
    # grant privilege
    chmod +x ~/start-hadoop.sh && \
    chmod +x ~/run-wordcount.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-dfs.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-yarn.sh

# format namenode
RUN ${HADOOP_HOME}/bin/hdfs namenode -format

#CMD [ "/usr/sbin/sshd", "-D"; bash"]
CMD [ "sh", "-c", "service ssh start; bash"]
```

这里使用debian系统作为基础镜像，也可以使用openjdk:8u212-jre-alpine3.9作为基础镜像，若使用后者作为基础镜像，则需要考虑时区同步问题：

```
FROM openjdk:8u212-jre-alpine3.9

MAINTAINER JohnWey


# 设置一些系统环境变量（字符集和时区）
ENV LANG en_US.UTF-8
ENV LANGUAGE en_US:en
ENV LC_ALL en_US.UTF-8
ENV TZ=Asia/Shanghai

# 换国内的源
# RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories
RUN echo "http://mirrors.aliyun.com/alpine/latest-stable/main/" > /etc/apk/repositories
RUN echo "http://mirrors.aliyun.com/alpine/latest-stable/community/" >> /etc/apk/repositories

# 安装一些必需的软件包，同步时区
RUN apk update && \
    apk add --no-cache wget bash openrc openssh-server tzdata && \
    cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime && \
    sed -i "s/#PermitRootLogin.*/PermitRootLogin yes/g" /etc/ssh/sshd_config && \
    ssh-keygen -t rsa -P "" -f /etc/ssh/ssh_host_rsa_key && \
    ssh-keygen -t ecdsa -P "" -f /etc/ssh/ssh_host_ecdsa_key && \
    ssh-keygen -t ed25519 -P "" -f /etc/ssh/ssh_host_ed25519_key


ENV HADOOP_VERSION 3.2.1

# install hadoop
ADD hadoop-${HADOOP_VERSION}.tar.gz /opt

# set environment variable
ENV JAVA_HOME=/usr/lib/jvm/java-1.8-openjdk
ENV HADOOP_HOME=/opt/hadoop-${HADOOP_VERSION}
ENV PATH=$PATH:${JAVA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
RUN ln -s /opt/hadoop-$HADOOP_VERSION/etc/hadoop /etc/hadoop
ENV HADOOP_CONF_DIR=/etc/hadoop


# ssh without key
RUN ssh-keygen -t rsa -f ~/.ssh/id_rsa -P '' && \
    cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys


# create directory
RUN mkdir -p /hadoop-data/dfs/name && \
    mkdir -p /hadoop-data/dfs/data && \
    mkdir $HADOOP_HOME/logs

COPY config/* /tmp/

RUN mv /tmp/ssh_config ~/.ssh/config && \
    mv /tmp/hadoop-env.sh ${HADOOP_HOME}/etc/hadoop/hadoop-env.sh && \
    mv /tmp/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/hdfs-site.xml && \
    mv /tmp/core-site.xml ${HADOOP_HOME}/etc/hadoop/core-site.xml && \
    mv /tmp/mapred-site.xml ${HADOOP_HOME}/etc/hadoop/mapred-site.xml && \
    mv /tmp/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/yarn-site.xml && \
    mv /tmp/workers ${HADOOP_HOME}/etc/hadoop/workers && \
    mv /tmp/start-hadoop.sh ~/start-hadoop.sh && \
    mv /tmp/stop-hadoop.sh ~/stop-hadoop.sh && \
    mv /tmp/run-wordcount.sh ~/run-wordcount.sh && \
    rm -rf /tmp && \
    sed -i 's/mesg n || true/tty -s \&\& mesg n/g'  /root/.profile && \
    # grant privilege
    chmod +x ~/start-hadoop.sh && \
    chmod +x ~/run-wordcount.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-dfs.sh && \
    chmod +x ${HADOOP_HOME}/sbin/start-yarn.sh

# format namenode
RUN ${HADOOP_HOME}/bin/hdfs namenode -format

#CMD [ "/usr/sbin/sshd", "-D"; bash"]
CMD [ "sh", "-c", "service ssh start; bash"]

```
上述配置文件配置如下

> core-site.xml：
```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop-bimaster:9000</value>
    </property>

    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/home/hadoop/hdfs/tmp</value>
    </property>

    <property>
        <name>io.file.buffer.size</name>
        <value>131072</value>
    </property>
    <! -- beeline connect(hive) -->
    <property>
	<name>hadoop.proxyuser.root.hosts</name>
	<value>*</value>
    </property>
    <property>
	<name>hadoop.proxyuser.root.groups</name>
	<value>*</value>
    </property>

</configuration>
```
> hdfs-site.xml
```
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
            Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
 <property>
    <name>dfs.http.address</name>
    <value>0.0.0.0:50070</value>
 </property>
 <property>
   <name>dfs.replication</name>
   <value>1</value>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/home/hadoop/hdfs/name</value>
   <final>true</final>
</property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/home/hadoop/hdfs/data</value>
   <final>true</final>
 </property>
 <property>
  <name>dfs.namenode.secondary.http-address</name>
   <value>hadoop-bimaster:19001</value>
 </property>
 <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
 </property>
 <property>
   <name>dfs.permissions</name>
   <value>false</value>
 </property>
</configuration>

```
> mapred-site.xml
```
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

<property>
    <name>mapreduce.application.classpath</name>
    <value>
        /opt/hadoop-3.2.1/etc/hadoop,
        /opt/hadoop-3.2.1/share/hadoop/common/*,
        /opt/hadoop-3.2.1/share/hadoop/common/lib/*,
        /opt/hadoop-3.2.1/share/hadoop/hdfs/*,
        /opt/hadoop-3.2.1/share/hadoop/hdfs/lib/*,
        /opt/hadoop-3.2.1/share/hadoop/mapreduce/*,
        /opt/hadoop-3.2.1/share/hadoop/mapreduce/lib/*,
        /opt/hadoop-3.2.1/share/hadoop/yarn/*,
        /opt/hadoop-3.2.1/share/hadoop/yarn/lib/*
    </value>
</property>
</configuration>
```
> yarn-site.xml
```
<?xml version="1.0"?>
<!--
       Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<configuration>
 <property>
 <name>yarn.resourcemanager.address</name>
   <value>hadoop-bimaster:18040</value>
 </property>
 <property>
   <name>yarn.resourcemanager.scheduler.address</name>
   <value>hadoop-bimaster:18030</value>
 </property>
 <property>
   <name>yarn.resourcemanager.webapp.address</name>
   <value>hadoop-bimaster:18088</value>
 </property>
 <property>
   <name>yarn.resourcemanager.resource-tracker.address</name>
   <value>hadoop-bimaster:18025</value>
 </property>
 <property>
   <name>yarn.resourcemanager.admin.address</name>
   <value>hadoop-bimaster:18141</value>
 </property>
 <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
 </property>
 <property>
     <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
     <value>org.apache.hadoop.mapred.ShuffleHandler</value>
 </property>
</configuration>
```
> workers
```
hadoop-bimaster
hadoop-bislaver01
hadoop-bislaver02
```
> ssh_config
```
Host localhost
  StrictHostKeyChecking no

Host 0.0.0.0
  StrictHostKeyChecking no

Host hadoop-*
   StrictHostKeyChecking no
```
> hadoop-env.sh
```
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HDFS_NAMENODE_USER=root
export HDFS_DATANODE_USER=root
export HDFS_SECONDARYNAMENODE_USER=root
export YARN_RESOURCEMANAGER_USER=root
export YARN_NODEMANAGER_USER=root
```
> start-hadoop.sh
```
#!/bin/bash

echo -e "\n"

$HADOOP_HOME/sbin/start-dfs.sh

echo -e "\n"

$HADOOP_HOME/sbin/start-yarn.sh

echo -e "\n"
```
> stop-hadoop.sh
```
#!/bin/bash

echo -e "\n"

$HADOOP_HOME/sbin/stop-yarn.sh

echo -e "\n"

$HADOOP_HOME/sbin/stop-dfs.sh

echo -e "\n"
```
> run-wordcount.sh
```
#!/bin/bash

# test the hadoop cluster by running wordcount

# create input files
mkdir input
echo "Hello Docker" >input/file2.txt
echo "Hello Hadoop" >input/file1.txt

# create input directory on HDFS
hadoop fs -mkdir -p input

# put input files to HDFS
hdfs dfs -put ./input/* input

# run wordcount
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar wordcount input output

# print the input files
echo -e "\ninput file1.txt:"
hdfs dfs -cat input/file1.txt

echo -e "\ninput file2.txt:"
hdfs dfs -cat input/file2.txt

# print the output of wordcount
echo -e "\nwordcount output:"
hdfs dfs -cat output/part-r-00000
```

使用下面的shell创建3个docker并搭建专用网络：
> start_container
```
#!/bin/bash

# the default node number is 3
N=${1:-3}


# start hadoop master container
sudo docker rm -f hadoop-bimaster &> /dev/null
echo "start hadoop-bimaster container..."
sudo docker run -itd \
                --net=hadoop \
                -p 9000:9000 \
                -p 50070:50070 \
                -p 18088:18088 \
                --name hadoop-bimaster \
                --hostname hadoop-bimaster \
                bidw/hadoop:base_v1.0 &> /dev/null


# start hadoop slave container
i=1
while [ $i -lt $N ]
do
	sudo docker rm -f hadoop-bislaver0$i &> /dev/null
	echo "start hadoop-bislaver0$i container..."
	sudo docker run -itd \
	                --net=hadoop \
	                --name hadoop-bislaver0$i \
	                --hostname hadoop-bislaver0$i \
	                bidw/hadoop:base_v1.0 &> /dev/null
	i=$(( $i + 1 ))
done

# get into hadoop master container
sudo docker exec -it hadoop-bimaster bash
```

搭建hadoop网络：
```
docker create network --driver=bridge hadoop
```
删除hadoop网络：
```
docker network rm hadoop
```

</div>



#### 2. 搭建Hive3.1.2

<div class="article__content" markdown="1">

```
FROM bidw/hadoop:base_v1.0

MAINTAINER JohnWey

ENV HADOOP_VERSION 3.2.1
ENV HIVE_VERSION 3.1.2

# install hive
ADD apache-hive-${HIVE_VERSION}-bin.tar.gz /opt
RUN mv /opt/apache-hive-${HIVE_VERSION}-bin /opt/hive


# set environment variable
ENV HADOOP_HOME=/opt/hadoop-${HADOOP_VERSION}
ENV HIVE_HOME=/opt/hive
ENV HIVE_CONF_DIR=${HIVE_HOME}/conf
ENV PATH=$PATH:${JAVA_HOME}/bin:${HIVE_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin


COPY mysql-connector-java-5.1.47.jar ${HIVE_HOME}/lib

COPY config/* /tmp/

#Custom configuration goes here
RUN mv /tmp/hive-site.xml $HIVE_HOME/conf  && \
    mv /tmp/beeline-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp//hive-env.sh $HIVE_HOME/conf && \
    mv /tmp/hive-exec-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp/hive-log4j2.properties $HIVE_HOME/conf && \
    mv /tmp/ivysettings.xml $HIVE_HOME/conf && \
    mv /tmp/llap-daemon-log4j2.properties $HIVE_HOME/conf && \
    mv ${HIVE_HOME}/lib/guava-19.0.jar ${HIVE_HOME}/lib/guava-19.0.jar_bak && \
    cp ${HADOOP_HOME}/share/hadoop/common/lib/guava-27.0-jre.jar ${HIVE_HOME}/lib/ && \
    mv /tmp/startup_hive.sh /usr/local/bin/ && \
    rm -rf /tmp && \
    chmod +x /usr/local/bin/startup_hive.sh

WORKDIR /usr/local/bin

#CMD [ "/usr/sbin/sshd", "-D"; bash"]
CMD [ "sh", "-c", "service ssh start; bash"]
```
> hive-site.xml
```
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
	<property>
		<name>javax.jdo.option.ConnectionURL</name>
		 <value>jdbc:mysql://hive-mysql:3306/hive?createDatabaseIfNotExist=true&amp;useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionDriverName</name>
		<value>com.mysql.jdbc.Driver</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionUserName</name>
		<value>root</value>
	</property>
	<property>
		<name>javax.jdo.option.ConnectionPassword</name>
		<value>yzw@123</value>
	</property>
	<property>
		<name>datanucleus.readOnlyDatastore</name>
		<value>false</value>
	</property>
	<property>
		<name>datanucleus.fixedDatastore</name>
		<value>false</value>
	</property>
	<property>
		<name>datanucleus.autoCreateSchema</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.schema.autoCreateAll</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.autoCreateTables</name>
		<value>true</value>
	</property>
	<property>
		<name>datanucleus.autoCreateColumns</name>
		<value>true</value>
	</property>
	<property>
		<name>hive.metastore.local</name>
		<value>true</value>
	</property>
	<!-- 显示表的列名 -->
	<property>
		<name>hive.cli.print.header</name>
		<value>true</value>
	</property>
	<!-- 显示数据库名称 -->
	<property>
		<name>hive.cli.print.current.db</name>
		<value>true</value>
	</property>
</configuration>
```
> hive-env.sh
```
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Set Hive and Hadoop environment variables here. These variables can be used
# to control the execution of Hive. It should be used by admins to configure
# the Hive installation (so that users do not have to set environment variables
# or set command line parameters to get correct behavior).
#
# The hive service being invoked (CLI/HWI etc.) is available via the environment
# variable SERVICE


# Hive Client memory usage can be an issue if a large number of clients
# are running at the same time. The flags below have been useful in
# reducing memory usage:
#
# if [ "$SERVICE" = "cli" ]; then
#   if [ -z "$DEBUG" ]; then
#     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseParNewGC -XX:-UseGCOverheadLimit"
#   else
#     export HADOOP_OPTS="$HADOOP_OPTS -XX:NewRatio=12 -Xms10m -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:-UseGCOverheadLimit"
#   fi
# fi

# The heap size of the jvm stared by hive shell script can be controlled via:
#
# export HADOOP_HEAPSIZE=1024
#
# Larger heap size may be required when running queries over large number of files or partitions.
# By default hive shell scripts use a heap size of 256 (MB).  Larger heap size would also be
# appropriate for hive server (hwi etc).


# Set HADOOP_HOME to point to a specific hadoop install directory
HADOOP_HOME=/opt/hadoop-3.2.1

# Hive Configuration Directory can be controlled by:
export HIVE_CONF_DIR=/opt/hive/conf

# Folder containing extra ibraries required for hive compilation/execution can be controlled by:
export HIVE_AUX_JARS_PATH=/opt/hive/lib
```
> start-up.sh
```
#!/bin/bash


hadoop fs -mkdir       /tmp
hadoop fs -mkdir -p    /user/hive/warehouse
hadoop fs -chmod g+w   /tmp
hadoop fs -chmod g+w   /user/hive/warehouse

schematool -dbType mysql -initSchema

#cd $HIVE_HOME/bin
#./hiveserver2 --hiveconf hive.server2.enable.doAs=false
```

运行hive container:
```
#!/bin/bash

# start hive container
sudo docker rm -f hadoop-hive &> /dev/null
echo "start hadoop-hive container..."
sudo docker run -itd \
                --net=hadoop \
                --name hadoop-hive \
                --hostname hadoop-hive \
                bidw/hive:base_v1.0
```
</div>


#### 3. 搭建MySQL
直接拉取Mysql镜像即可：
```
docker pull mysql:5.7
```
运行镜像：
```
docker run -itd --net=hadoop --name hive-mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=yzw@123 mysql:5.7
```


#### 4. QA
Q1：在运行Hive时，报错：java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument(ZLjava/lang/String;Ljava/lang/Object;)V)
A1：这是jar版本的问题所致，解决方法已在DockerFile里体现
    mv ${HIVE_HOME}/lib/guava-19.0.jar ${HIVE_HOME}/lib/guava-19.0.jar_bak
    copy ${HADOOP_HOME}/share/hadoop/common/lib/guava-27.0-jre.jar ${HIVE_HOME}/lib/guava-27.0-jre.jar
Q2：使用beeline连接Hive，“!connect jdbc:hive2://localhost:10000/default”时，报错：WARN jdbc.HiveConnection: Failed to connect to localhost:10000
A2：这是由于权限所致，需在core-site.xml中加入：
```
    <! -- beeline connect(hive) -->
    <property>
	<name>hadoop.proxyuser.xxx.hosts</name>
	<value>*</value>
    </property>
    <property>
	<name>hadoop.proxyuser.xxx.groups</name>
	<value>*</value>
    </property>
```
其中xxx是连接hive使用的用户，如root。




